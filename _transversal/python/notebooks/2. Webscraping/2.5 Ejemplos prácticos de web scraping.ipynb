{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extracción del texto de un discurso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de la librería pandas\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://elpais.com/internacional/2018/01/31/actualidad/1517387619_036241.html'\n",
    "html = requests.get(url)\n",
    "soup2 = BeautifulSoup(html)\n",
    "\n",
    "tags = soup2('p') # Extraigo las etiquetas de tipo <p>\n",
    "discurso = '' # Aquí guardo el discurso, inicialmente vacío\n",
    "\n",
    "for tag in tags:\n",
    "    if (len(tag.attrs)) == 0: # Con esta condición filtro solo el texto que me interesa\n",
    "        a= tag.contents[0] # Extraigo el contenido de la etiqueta\n",
    "        discurso = discurso +  a # Voy concatenando cada contenido a la variable discurso\n",
    "\n",
    "#print(discurso)\n",
    "\n",
    "contadores = dict() # Creo un diccionario para almacenar las palabras y contar las veces que aparece\n",
    "discurso = discurso.replace(',','').replace('.','').replace(':','').replace('?','').replace('(','').replace(')','') # Elimino signos de (puntos, comas, etc.)\n",
    "palabras = discurso.lower().split() # Paso todas las palabras a minúsculas (lower) y las corto palabra a palabra (split)\n",
    "for palabra in palabras:\n",
    "    if len(palabra)>3: # Añado al diccionario solo las palabras con más de 3 letras\n",
    "        contadores[palabra] = contadores.get(palabra,0) + 1 \n",
    "        \n",
    "        \n",
    "import pandas as pd\n",
    "# Guardo las palabras en un dataframe ordenados de mayor a menor por el contador\n",
    "pd.DataFrame(list(contadores.items()), columns=['palabra', 'contador']).sort_values('contador', ascending=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extracción de valores de un fondo de inversión "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extraer datos de una web tenemos que analizar el código fuente. Si inspeccionamos el valor del fondo vemos que tiene esta etiqueta: \n",
    "```html\n",
    "<td class=\"line text\">EUR 15.15</td>\n",
    "```\n",
    "\n",
    "Por lo tanto, voy a utilizar BeautifulSoup para extraer las etiquetas `\"td\"` con `class = \"line text\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849.0\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "#iShares Dev Rl Ett Idx (IE) Instl Acc €\n",
    "participaciones = 50 # Estas son mis participaciones en el fondo de inversión\n",
    "url= 'http://www.morningstarfunds.ie/ie/funds/snapshot/snapshot.aspx?id=F00000PJME'\n",
    "html=urllib.request.urlopen(url)\n",
    "soup=BeautifulSoup(html)\n",
    "tags = soup.find_all(\"td\",class_=\"line text\") # Extraigo las etiquetas \n",
    "valor = float(tags[0].contents[0].replace('EUR\\xa0','')) # Me quedo únicamente con el valor numérico\n",
    "total = participaciones * valor\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Web scraping de laliga.es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos los hashtags de los próximos encuentros  \n",
    "<img src='partidos2.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.laliga.com/laliga-santander/clubes'\n",
    "html = urllib.request.urlopen(url)\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# Si inspeccionamos la página podemos ver cómo esa clase es la asignada a cada equipo\n",
    "#tags = soup.find_all(\"h2\",class_=\"styled__TextHeaderStyled-sc-1edycnf-0 bYHKtg\") #El problema de esta solución, es que busca un atributo específico\n",
    "regex = re.compile('styled__TextHeaderStyled-sc-1edycnf-0')  \n",
    "tags = soup.find_all(\"h2\", {\"class\": regex}) #Esta solución, puede buscar una coincidencia parcial\n",
    "\n",
    "hashtags = list()\n",
    "teams = dict()\n",
    "\n",
    "for tag in tags:\n",
    "    hashtags.append(tag.contents[0])\n",
    "   # print(tag.find_parent('a'))\n",
    "    tagParentTeam = tag.find_parent('a')\n",
    "    print(tagParentTeam['href'])\n",
    "    teams[tag.contents[0]] = tagParentTeam['href']\n",
    "\n",
    "\n",
    "#print(hashtags)\n",
    "#print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para navegar a cada equipo, se necesita obtener la url-del-equipoo\n",
    "urlbase = 'https://www.laliga.com'\n",
    "for teamName,urlTeamPartial in teams.items():\n",
    "    urlTeam = urlbase+urlTeamPartial;\n",
    "    print(urlTeam)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar cada equipo\n",
    " # definir una función para procesar cada equipo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
